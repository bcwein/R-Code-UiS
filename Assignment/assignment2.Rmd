---
title: "Assignment 2"
author: "Bj√∏rn Christian Weinbach"
date: \today
header-includes:
   - \usepackage{bbm}
   - \usepackage[UKenglish]{babel}
   - \usepackage[T1]{fontenc}
   - \usepackage[nodayofweek,level]{datetime}
urlcolor: blue
output: pdf_document
bibliography: assignment2.bib
---

Clear R environment
```{r}
rm(list = ls())
```

# Problem 1

Consider the integral

$$
\int_{-1}^{1} \int_{-1}^{1} 1_D(x,y) dxdy
$$

Where $1_D(x, y)$ is the indicator function defined so that 
$$
1_D(x,y) = \begin{cases}
1 \quad \text{if} \quad x^2 + y^2 \leq 1, \newline
0 \quad \text{otherwise.}
\end{cases}
$$
As a crude first attempt, consider the estimator
$$
\theta_{CMC} = \frac{4}{N}\sum_{i=1}^{n} 1_D(X_i, Y_i)
$$
A.) Argue for why $\theta_{CMC}$ is a monte carlo estimator for the integral above.

According to [this](https://en.wikipedia.org/wiki/Monte_Carlo_integration#overview) 
wikipedia article which  was accessed the \formatdate{09}{10}{2020}, 
the monte carlo estimate for a multidimensional definite integral [@wiki:mcint]


$$
I = \int_\Omega f(\boldsymbol{\bar{x}}) d\boldsymbol{\bar{x}}
$$

where $\Omega$ is a subset of $R^m$, has volume


$$
V = \int_\Omega d\boldsymbol{\bar{x}}
$$

The naive Monte Carlo approach is to sample points uniformly on $\Omega$
given N uniform samples,

$$
\boldsymbol{\bar{x}_1}, \dots, \boldsymbol{\bar{x}_n} \in \Omega,
$$
I can be approximated by
$$
I \approx \Omega_N \equiv  V \frac{1}{N}\sum_{i=1}^{N} f(\boldsymbol{\bar{x}_i})
$$

Which is true due to the law of large numbers.

In our case, which is also very similar to the 
[example](https://en.wikipedia.org/wiki/Monte_Carlo_integration#Example) on the 
wikipedia article for monte carlo integration, $\Omega = [-1, 1] \times [-1, 1]$
with $V = \int_{-1}^1 \int_{-1}^1 dxdy = 4$ which gives the following crude way 
to estimate $I$ [@wiki:mcint]
$$
I = \frac{4}{N}\sum_{i=1}^{N}1_D(X_i, Y_i) 
$$

Which is the proposed estimator $\theta_{CMC}$.

A.) Show that $1_D(X_i, Y_i)$ has a bernoulli distribution with $p=\frac{\pi}{4}$

The function returns 1 / success or 0 / failure, and is therefore has a potential
bernoulli distrbution. To calculate $P(X^2 + Y^2 \leq 1)$ we need to calculate
the integral

$$
P({X,Y} \in X^2 + Y^2 \leq 1) = \int_A f_{XY}(x,y) dx dy
$$
Where  $A$ is the region of the $X^2 + Y^2 \leq 1$ i.e disc with radius 1.
The pdf of a bivariate uniform distribution is a single constant c. 
To find this constant, we use the fact that the probability of a point being 
inside the sample space $\Omega$ is 1 and

$$
\int_\Omega f_{XY}(x, y) dx dy = 1 \implies c \int_\Omega dxdy = 1
$$
Where the integral is the area of the sample space and we get 
$$
c \cdot \text{area of square} = 1 \implies c = \frac{1}{\text{area of square}}
\implies f_{xy} = c = \frac{1}{4}
$$
and then the integral of a point being inside the unit square is

$$
\int_A f_{XY}(x,y) dx dy = c \cdot \text{area of circle}
$$
and we get $\frac{\pi}{4}$. The indicator function therefore is a bernoulli
distribution with probability $\frac{\pi}{4}$ of being inside the unit circle.

b.) Which distribution does $Z = \sum_{i=1}^{n} 1_d(X_i, Y_i)$ have

If we sum N independent bernoulli samples/trials with constant p we have a
binomial distribution. P is constant as discussed above and the trials are
independent due to $X$ and $Y$ being independent uniform variables. 

The sum is therefore a valid binomial distributed random variable 
$Z \sim B(n, \frac{\pi}{4})$. The expected value of a binomial distribution is
$E[Y] = n \cdot p = \frac{n\pi}{4}$

The expected value of the estimator $\theta_{CMC}$ is then

$$
E[\theta_{CMC}] = \frac{4}{n} \cdot Y = \frac{4}{n} \cdot E[Y] = \frac{4}{n}
\cdot \frac{n\pi}{4} = \pi
$$
And we see that the the estimator has expected value $\pi$.

The variance of the estimator $\theta_{CMC}$ is
$$
Var(\theta_{CMC}) = Var(\frac{4}{n}Y) = \frac{16}{n^2}Var(Y) = 
\frac{16}{n^2} n\frac{\pi}{4}(1-\frac{\pi}{4}) = 
\frac{4(1- \frac{\pi}{4})\pi}{n}
$$

c) Implementation of monte carlo estimate of $\theta_{CMC}$ with $N = 1000$
 
```{r}
indicator1 <- function (x, y) {          # Indicator function for unit circle
  return((x^2+y^2 <= 1))
}

mcEstimatePi <- function (Sims) {        # function for monte carlo estimate 
  mcPi <- numeric(Sims)
  for (i in 1:Sims) {
    N <- 1000                            # Number of samples
    x <- runif(N, -1, 1)                 # x values from uniform
    y <- runif(N, -1, 1)                 # y values from uniform
    mcPi[i] <- (4/N)*sum(indicator1(x, y)) # return monte carlo estimate
  }
  return(mcPi)
}

Sims <- 10000                            # Simulations of pi
mcPi <- mcEstimatePi(Sims)               # Function call
summary(mcPi)                            # Summary stats for MC estimate
hist(mcPi, probability = TRUE)           # Histogram
lines(density(mcPi))                     # Plot empirical density
```
```{r}
mean <- mean(mcPi)
var <- var(mcPi)
exact_mean <- pi
exact_var <- (4*(1 - (pi/4))*pi / Sims)
print(paste("Simulated mean: ", mean))
print(paste("Exact mean: ", exact_mean))
print(paste("Simulated var: ", var))
print(paste("Exact var: ", exact_var))
```

We see that the distribution is approxamately normally distrobuted with sample
mean at approx $Mean = 3.141$ and sample variance at approx $Variance =  0.00260$.
This is due to the central limit theorem because our estimate of pi is based on a
sum of several samples.

d.) Calculate probability of correctly estimating to two decimal places

```{r}
# count average of booleans (probability) of estimatesfrom 3.14 to 3.149999...
mean(mcPi < 3.15) - mean(mcPi <= 3.14)
```
e.) Introducing antithetic variables

The assignment proposes two antithetic variables $V = a + b - X = -X$ and 
$W = -Y$

These will not reduce the monte carlo variance since $X$ and $Y$ are independent
uniform random variables and due to their independence there is no negative
correlation gained by just flipping the sign of both variables from $X$ and $Y$ to  
$-X$ and $-Y$.
```{r}
x <- runif(1000, -1, 1)
y <- runif(1000, -1, 1)
cov(-x, -y)
```

f.) Let's see if our variance is reduced by doing exercise c. with the new variables.

```{r}
mcEstimatePi <- function (Sims) {        # function for monte carlo estimate 
  mcPi <- numeric(Sims)
  for (i in 1:Sims) {
    N <- 1000                            # Number of samples
    x <- runif(N, -1, 1)                 # x values from uniform
    y <- runif(N, -1, 1)                 # y values from uniform
    mcPi[i] <- (4/N)*sum(indicator1(-x, -y)) # mcEstimate with new variables
  }
  return(mcPi)
}

Sims <- 10000                            # Simulations of pi
mcPi <- mcEstimatePi(Sims)               # Function call
summary(mcPi)                            # Summary stats for MC estimate
mean(mcPi)
var(mcPi)
```
g.) Check if shift function reduces variance

```{r}
shift <- function(u) {                   # Introducing shift function
  return(((u+2.0) %% 2.0) - 1.0)
}

mcEstimatePi <- function (Sims) {        # function for monte carlo estimate 
  mcPi <- numeric(Sims)
  for (i in 1:Sims) {
    N <- 1000                            # Number of samples
    x <- runif(N, -1, 1)                 # x values from uniform
    y <- runif(N, -1, 1)                 # y values from uniform
    sx <- shift(x)
    sy <- shift(y)
    mcPi[i] <- (4/N)*sum(indicator1(sx, sy)) # mcEstimate with new variables
  }
  return(mcPi)
}

Sims <- 10000                            # Simulations of pi
mcPi <- mcEstimatePi(Sims)               # Function call
summary(mcPi)                            # Summary stats for MC estimate
mean(mcPi)
var(mcPi)
```
h.) Use important sampling

```{r}
# zero mean independent Normal
f <- function (x, y, sigma) {
  const <- (1/(2*pi*sigma^2))
  exp1 <- exp(-x^2/(2*sigma^2))
  exp2 <- exp(-x^2/(2*sigma^2))
  return(const*exp1*exp2)
}

mcEstimatePi <- function (Sims, sigma) {    # function for monte carlo estimate 
  mcPi <- numeric(Sims)
  for (i in 1:Sims) {
    N <- 1000                                 # Number of samples
    x <- rnorm(N, 0, sigma^2)                 # x values from uniform
    y <- rnorm(N, 0, sigma^2)                 # y values from uniform
    mcPi[i] <- mean(indicator1(x, y) / f(x, y, sigma))
  }
  return(mcPi)
}
```

```{r}
sigmas <- c(0.3, 0.624127, 1.0)

for (s in sigmas) {
  mcPi <- mcEstimatePi(10000, s)
  print(summary(mcPi))
}
```
The estimators are not that good and i was not anble to replicate the resutls
described in the assignment, which makes my findings quite uncertain. My best
answer to why this estimator did not work is mainly due to tail behaviour and 
$f(x)$ not being an appropriate function for this use. 

But due to the fact that i was not able to replicate the results also suggests
a falty implementation above.

# Problem 2

Define $\lambda(t)$ in R 
```{r}
# Specify the intensity function for storms
lambdastorm <- function(t) {
 (297 / 10)*(1 + cos(2*pi*(t + (1/10)))) * (1 - (exp(-t/10)/2)) + 3/5
}

lambdastorm(0.5)
```

a.) Calculating number of storms, expected value and variance

According to Rizzo on page 103. A poisson proccess with a intensity function
$\lambda(t)$ has the property that the number of events $N(t)$ in interval $[0, t]$ 
has the poisson distribution with mean 
$$
E[N(t)] = \int_0^t \lambda(y) dy
$$
Which in our case gives 

```{r}
integrate(lambdastorm, 0, 1)
```

Let's find the expected number of storms in 2025 by calculating the integral
```{r}
integrate(lambdastorm, 5, 6)
```

And let's find the expected value and standard deviation of storms in 
2020 and 2021 combined

Expected value is calculated using the integral below
```{r}
integrate(lambdastorm, 0, 2)
```

Which means the number of events in 2020 and 2021 combined is poisson
distrubuted with $\lambda = 39.72776$. The variance of a poisson distribution is
$Var(X) = \lambda$ (according to Rizzo on page 44).  
$$
SD(X) = \sqrt{Var(X)} = \sqrt{33.92776 } =  5.824754
$$
b.) Find smallest possible $\lambda_{max}$ for all $\lambda(t), \quad t \geq 0$

The function $\lambda(t)$ does not have a global maximum because it is modeled
with a increasing winter intensity due to climate change that does not stop
increasing. Solving for $\frac{d}{dt} \lambda(t) = 0$ gives an infinite number
of potential maximum or minimum points and there is no $\lambda_{max}$ for all
$\lambda(t)$ values.

c.) Validate previous points by simulation

simtNHPP borrowed from lectures on stochastic processes
```{r}
# Function for simulating arrival times for a NHPP between a and b using thinning
simtNHPP <- function(a,b,lambdamax,lambdafunc){
  # Simple check that a not too small lambdamax is set
  if(max(lambdafunc(seq(a,b,length.out = 100)))>lambdamax)
    stop("lambdamax is smaller than max of the lambdafunction")
  
  # First simulate HPP with intensity lambdamax on a to b
  expectednumber <- (b-a)*lambdamax
  # Simulate more than the expected number to be certain to exceed stoptime
  Nsim <- 3*expectednumber
  # Simulate interarrival times
  timesbetween <- rexp(Nsim,lambdamax)
  # Calculate arrival times starting at a
  timesto <- a+cumsum(timesbetween)
  # Discard the times larger than b
  timesto <- timesto[timesto<b]
  # Count the number of events
  Nevents <- length(timesto)
  # Next do the thinning. Only keep the times where u<lambda(s)/lambdamax
  U <- runif(Nevents)
  timesto <- timesto[U<lambdafunc(timesto)/lambdamax]  
  timesto  # Return the remaining times
}

Nsim <- 1000
a <- 0
b <- 1
NHPPnumbers <- vector(length=Nsim)
for(i in 1:Nsim) {
  NHPPnumbers[i] <- length(simtNHPP(a=a,b=b,
                           lambdamax=max(lambdastorm(seq(a, b, 0.01))),
                           lambdafunc=lambdastorm))
}

# Exepcted number of storms in 2020
mean(NHPPnumbers)

Nsim <- 1000
a <- 5
b <- 6
NHPPnumbers <- vector(length=Nsim)
for(i in 1:Nsim) {
  NHPPnumbers[i] <- length(simtNHPP(a=a, b=b,
                           lambdamax=max(lambdastorm(seq(a, b, 0.01))),
                           lambdafunc=lambdastorm))
}

# Exepcted number of storms in 2025
mean(NHPPnumbers)

Nsim <- 1000
a <- 0
b <- 2
NHPPnumbers <- vector(length=Nsim)
for(i in 1:Nsim) {
  NHPPnumbers[i] <- length(simtNHPP(a=a, b=b,
                           lambdamax=max(lambdastorm(seq(a, b, 0.01))),
                           lambdafunc=lambdastorm))
}

# Expected number of storms in 2020 and 2021
mean(NHPPnumbers)
# Variance of number of storms in 2020 and 2021
var(NHPPnumbers)
# Standard deviation of number of storms in 2020 and 2021
sd(NHPPnumbers)
```
d.) Simulate claim size

To calculate the claim size for a given year, simulate a poisson process
and calculate mean parameter for all storms, then draw claim size for
exponential distribution with mean parameter $c(t_i)$ where $t_i$ is time of a 
given storm simulated from the NHPP.
```{r}
# mean function
claim <- function (t) {
  return(10*exp(5*t / 100))
}

# function for simulatin Nsim claims, from a to b
simulate_claims <- function(Nsim, a, b) {
  Claims <- vector(length=Nsim)
  for(i in 1:Nsim) {
    # Calculate mean parameter of storm based on time of storm
    expmean <- claim(simtNHPP(a=a, b=b,
                     lambdamax=max(lambdastorm(seq(a, b, 0.01))),
                     lambdafunc=lambdastorm))
    # Draw claim size for all storms from exponential with calculated mean param
    Claims[i] <- sum(rexp(length(expmean), (1/expmean)))
  }
  return(Claims)
}

claims <- simulate_claims(10000, 0, 1)
hist(claims, breaks=20)
# Calculate mean
mean(claims)
# Calculate std
sd(claims)
```
To find a confidence interval one simple approach is to calculate the standard
normal confidence interval. This can be done since the distribution of means 
approach a normal distribution due to the central limit theorem.

```{r}
# Standard normal confidence interval
CI<- c(mean(claims) - qnorm(0.975, 0, 1)*(sd(claims)/sqrt(length(claims))),
       mean(claims) + qnorm(0.975, 0, 1)*(sd(claims)/sqrt(length(claims))))
CI
```

To be 97.5% certain to be able to be sure that the company is able to cover all
claims, 97.5% of the simulated costs must be possible to pay. I.e, the 97.5 
precentile of the simulated claims must be calculated

```{r}
quantile(claims, c(0.975))
```

And we see that 97.5% of the simulated costs are less than approx 300 million
kroners during 2020.

e.) Calculate claims using Rao-Blackwelliztion

R code for estimating $E[X] = E[Y]$ using equation 6 and 7 in assigment  2.

```{r}
# function for simulatin Nsim claims, from a to b
# Rao-Blackwelliztion approach
simulate_claims <- function(Nsim, a, b) {
  Claims <- vector(length=Nsim)
  for(i in 1:Nsim) {
    # Calculate mean parameter of storm based on time of storm and sum them
    Claims[i] <- sum(claim(simtNHPP(a=a, b=b,
                     lambdamax=max(lambdastorm(seq(a, b, 0.01))),
                     lambdafunc=lambdastorm)))
  }
  return(Claims)
}

claims <- simulate_claims(10000, 0, 1)
hist(claims, breaks=20)
# Calculate mean
mean(claims)
# Calculate std
sd(claims)
```

We observe that the standard deviation is smaller and by inspection see that the
histogram is not as wide as the previous estimate. Now, let's calculate the
confidence interval for the mean of claims.

```{r}
# Standard normal confidence interval
CI<- c(mean(claims) - qnorm(0.975, 0, 1)*(sd(claims)/sqrt(length(claims))),
       mean(claims) + qnorm(0.975, 0, 1)*(sd(claims)/sqrt(length(claims))))
CI
```

f.) Propose and implement improved estimator of $Var(X)$

# Problem 3

Load data and run simple regression
```{r}
load("prob23.dat")
lm.obj <- lm(y ~ x1+x2+x3+x4+x5,data=df)
Rsquared <- summary(lm.obj)$r.squared
Rsquared
```

a.) Calculate $B$ bootstrap samples for $R^2$ and plot histogram

Code below is borrowed both from bootstra_examples.R file from the lectures
as well as [this](https://www.statmethods.net/advstats/bootstrapping.html)
article by statmethods.net [@statsmetboot]

```{r}
# Bootstrap 95% CI for R-Squared
library(boot)

# function to obtain R-Squared from the data
rsq <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample
  fit <- lm(formula, data=d)
  return(summary(fit)$r.square)
}
# bootstrapping with 5000 replications
results <- boot(data=df, statistic=rsq, R=5000, formula=y~x1+x2+x3+x4+x5)

# view results
results
plot(results)

# get 95% confidence interval
boot.ci(results, type=c("bca"))
```
We see from the histogram, the quantile plot and the confidence intervals 
that the bootstrap samples of $R^2$ is fairly normal.

b.) Find bootstrap estimate for bias of $R^2$

According to the ordinary nonparametric bootstrap using the boot library,
$bias(R^2) \approx 0.00148$ and  $\text{std.err}(R)^2 \approx 0.0167$

b.) Calculate 99% confidence interval

To calculate the 99% confidence interval for $R^2$ using standard normal interval
and percentile interval, we use the boot library.

```{r}
boot.ci(results, conf=0.99, type=c("norm", "perc"))
```
We see that the confidence interval is not quite equal, the normal confidence
interval is a tiny bit wider thatn the percentile interval. This is due to 
the data not beeing quite normal. We can see this form the quantile plot above
where we see that $R^2$ values fart from the mean of the data is not as often
as expected from a standard normal distributition.

# Problem 4

According to [@burkardt2014truncated pp.20-24], the CDF of a general truncated
normal distribution bounded on the interval $[a, b]$ is

$$
\boldsymbol{\psi}(\bar{\mu}, \bar{\sigma}, a, b; x) = 
\begin{cases} 
  0 & x\leq a \\
  \frac{\boldsymbol{\phi}(\bar{\mu}, \bar{\sigma}^2; x) - 
        \boldsymbol{\phi}(\bar{\mu}, \bar{\sigma}^2; a)}
       {\boldsymbol{\phi}(\bar{\mu}, \bar{\sigma}^2; b) - 
        \boldsymbol{\phi}(\bar{\mu}, \bar{\sigma}^2; a)} & a < x < b \\
  0 & b\leq x 
\end{cases}
$$
Where $\boldsymbol{\phi}$ being the CDF of a normal distribution.

By replacing some of the notation to reflect the notation used in the
assignment, namely:

* replacing $x$ with $r$
* using $F$ for notating the CDF of a normal $\boldsymbol{\phi}$
* the fact that $b = +\infty$ and thus the $\boldsymbol{\phi}(b) = 1$
* replacing the left bound $a$ with $l$

We get

$$
G_R(l, \mu, \sigma; r) = 
\begin{cases}
  0 & r < l \\
  \frac{F(r) - F(l)}{1 - F()} & l \leq r
\end{cases}
$$
To find the inverse $G_R^{-1}(u)$ we replace $G$ with $U$ where $U$ represent
the $Uniform(0,1)$ distribution and solve for $r$

$$
U = 
\frac{F(r) - F(l)}{1 - F(l)} \implies F(r) = F(l) + U(1 - F(l))
\implies r = F^{-1}(F(l) + U(1 - F(l)))
$$

Which is the inverse cumulative distribution shown in the assignment.

b.) Inverse transform sampling of left-truncated normal distribution

The code below evaluates $G_R^{-1}(u)$ by using the fact that the inverse of
a CDF of a normal distribution is the quantile function of a normal distribution
and utilizes the definition of the pdf of a left-truncated normal distribution
defined by [@burkardt2014truncated pp.20]

```{r}
inverse_truncated <- function(samples, l, mu, sigma) {
  # Sample uniform numbers
  u <- runif(samples, min = 0, max = 1)
  # Cumulative normal distribution
  f <- function(x) pnorm(x, mu, sigma)
  # Inverse cumulative normal distribtution (quantile)
  f.inv <- function(x) qnorm(x, mu, sigma)
  # Inverse transform sampling
  r <- f.inv(f(l) + u*(1 - f(l)))
  return(r)
}

# Density of truncated normal distribution 
# Based on pdf by (Burkardt 2014, 20)
pdf_truncated <- function(x, l, mu, sigma) {
  return(dnorm(x, mu, sigma) / (1 - pnorm(l, mu, sigma)))
}


l <- 1.8
mu <- 2.2
sigma <- 1.5
x <- inverse_truncated(10000, l, mu, sigma)
hist(x, probability = TRUE,  xlab="r", breaks=50)
curve(pdf_truncated(x, l, mu, sigma), 0, max(x), lwd=2, xlab = "", ylab = "", add = T)
```

b.) Calculate the theoretical mean and standard deviation

The pdf as defined in the pdf_truncated function was not possible to integrate.
The function is defined again below and integrated.

```{r}
# r*Fr(r) R integral
expected <- integrate( function(r, l=1.8, mu=2.2, sigma=1.5) 
  {
    r*(exp(-(r-mu)^2 / (2*sigma^2))/(sqrt(2*pi*sigma^2)*
    (1-pnorm(l, mu, sigma))))*(r>= l)
  }, -Inf, Inf)$value

# r^2*Fr(r) R integral
expected_r2 <- integrate( function(r, l=1.8, mu=2.2, sigma=1.5) 
  {
    (r^2)*(exp(-(r-mu)^2 / (2*sigma^2))/(sqrt(2*pi*sigma^2)*
    (1-pnorm(l, mu, sigma))))*(r>= l)
  }, -Inf, Inf)$value

print(paste("Mean: ", expected))
print(paste("SD: ", sqrt(expected_r2 - expected^2)))
print(paste("Simulated mean: ", mean(x)))
print(paste("Simulated sd: ", sd(x)))
```
c.) Simulate R values and S values

```{r}
# No of samples
Samp <- 1000
# Simulate R from truncated normal
R <- inverse_truncated(Samp, l=1.8, mu=2.2, sigma=1.5)
# Simulate S conditionally on R
S <- runif(Samp, 0.5/R, 0.8/R)
# N x 2 matrix
m <- cbind(R, S)
plot(m)
```
To obtain the mean vector or **expected value vector** $E[\boldsymbol{X}]$ where
$$
\boldsymbol{X_i} = 
\begin{bmatrix}
R_i \\
S_i
\end{bmatrix} \qquad \text{for} \quad i = {1, 2, \dots, N}
$$
is a vector of R values and S values for N simulated vectors. We can estimate the
mean vector by estimating the individual means for each random variable in the 
vectors and we get

$$
E[\boldsymbol{X}] = 
\begin{bmatrix}
E[R] \\
E[S]
\end{bmatrix} =
\begin{bmatrix}
\frac{1}{N}\sum_{i=1}^NR_i \\
\frac{1}{N}\sum_{i=1}^NS_i \\
\end{bmatrix}
$$
In r:

```{r}
meanvec <- colMeans(m)
meanvec
```
To calculate the covariance matrix $K_{RS} = \text{cov}(R, S)$ and the correlation
matrix (which is the covariance matrix of standardized random variables). We
use the r fucntions cov and cor.

```{r}
cov(m)
cor(m)
```

d.) Histogram and distribution of S

We plot the values of S as an histogram and use the 
[R](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/density)
function for estimating the density to plot the estimated density against the
histogram.

```{r}
hist(S, breaks = 20, probability = TRUE)
lines(density(S))
```

And we visually can see that the density of S is **not** uniform.

Let's calculate the marginal density $f_s(s)$. The limits of integration utilizes
the support of the pdf $\frac{0.5}{R} \leq S$ to determine values of R when integrating.

R code below calculates the integral

$$
f_S(s) = \int_{\frac{0.5}{s}}^{\frac{0.8}{s}} f_{RS}(r, s) dr
$$

```{r}
# Marginal distribution of S
marginal_s <- function(s, l, mu, sigma) {
  integrate (
    function(r) {
      ((exp(-(r-mu)^2 / (2*sigma^2))*10*r) / 
      (sqrt(2*pi*sigma^2)*(1-pnorm(l, mu, sigma))*3))*(s >= 0.5/r)*(s<=0.8/r)
    }, 0.5/s, 0.8/s
  )$value
}

x <- S
x <- sort(x, decreasing = FALSE)
marginal <- numeric(length(x))
i <- 0
for (value in x) {
  i <- i+1
  marginal[i] <- marginal_s(value, 1.8, 2.2, 1.5)
}
hist(x, breaks = 20, probability = TRUE)
lines(x, marginal, col="red")
lines(density(S), col="blue")
```
And we see that the marginal density plotted in red fits the simulated values of S quite well.

e.) Generate R and S using antithetic variables.

We introduce an antithetic uniform variable when sampling R via the inverse
transform method. u1 and u2 are negatively correlated and variance
is reduced.
```{r}
inverse_truncated_anti <- function(samples, l, mu, sigma) {
  # Sample uniform numbers
  u1 <- runif(samples/2, min = 0, max = 1)
  u2 <- 1 - u1
  u <- c(u1, u2)
  # Cumulative normal distribution
  f <- function(x) pnorm(x, mu, sigma)
  # Inverse cumulative normal distribtution (quantile)
  f.inv <- function(x) qnorm(x, mu, sigma)
  # Inverse transform sampling
  r <- f.inv(f(l) + u*(1 - f(l)))
  return(r)
}

# No of samples
Samp <- 1000
# Simulate R from truncated normal
R <- inverse_truncated_anti(Samp, l=1.8, mu=2.2, sigma=1.5)
# Simulate S conditionally on R
S <- runif(Samp, 0.5/R, 0.8/R)
# N x 2 matrix
m <- cbind(R, S)
plot(m)
```
Still, we don't observe an significant reduction in the variance of the (R, S) values. 

e.) Calculate hospital beds needed

Import SEIR-model

```{r}
source("seir_model.R")
f <- seir_sim()
```

```{r}
source("seir_model.R") # assumed already run
nsim <- 200 # number of simulation replica
days <- 365 # 366 days in 2021.
population <- 10^6

t <- which(f$dates<="2021-12-31" & f$dates>="2021-01-01")
m <- matrix(,nsim, days)
for(i in 1:nsim){
  ff <- seir_sim(R0max = R[i],
                 soc_dist_Rfac = S[i])
  m[i,] <- population*(ff$ode[t,"HH"] + ff$ode[t,"HC"] + ff$ode[t,"CC"])
}

rowmax <- apply(m, 1, function(x) max(x))
hist(rowmax, probability = TRUE, breaks = 30, 
     main="Maximum number of required beds during 2021 pr 1000000")
```
f.) Uncertainty in daily required hospital beds

Plot cumulative sums of daily needed beds over 2021

```{r}
cummat <- matrix(,nsim, 365)
cl <- rainbow(200)

plot(0,0,xlim = c(0,365),ylim = c(0,70000), type = "n")
for (i in 1:nsim) {
  lines(cumsum(m[i,]), col=cl[i], type="l", lw=0.3)
}
```

We see that for our 200 simulations, the cumulative sum of daily beds needed
at the end of the year is in the range of 30000 - 70000.

\newpage

# Bibliography